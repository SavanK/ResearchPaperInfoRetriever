Evaluation of Hands-Free HMD-Based Navigation Techniques for Immersive Data Analysis
Daniel Zielasko* Sven Horn Sebastian Freitag* Benjamin Weyers* Torsten W. Kuhlen*
Visual Computing Institute, RWTH Aachen University JARA ­ High-Performance Computing

ABSTRACT
To use the full potential of immersive data analysis when wearing a head-mounted display, the user has to be able to navigate through the spatial data. We collected, developed and evaluated 5 different handsfree navigation methods that are usable while seated in the analyst's usual workplace. All methods meet the requirements of being easy to learn and inexpensive to integrate into existing workplaces. We conducted a user study with 23 participants which showed that a body leaning metaphor and an accelerometer pedal metaphor performed best within the given task.
Index Terms: H.5.2 [Information Interfaces and Presentation]: User Interfaces--Evaluation/methodology
1 INTRODUCTION
The analysis of spatial data sets can benefit from their inspection in Immersive Virtual Environments (IVEs) [7]. Head-Mounted Displays (HMDs) are a portable option to acess IVEs and are typically easy to install into a data analyst's usual workplace.
As navigation is a main requirement for data exploration in immersive data analysis, a technique is needed to enable the user to navigate in a head-mounted display setting. Classical device combinations for navigation, like mouse and keyboard, or devices like joysticks and gamepads are usually not an option, as they cannot be seen when wearing an HMD and additionally occupy the hands of the analyst. The latter prevents her from, e.g., manipulating objects in the virtual data space in a natural way at the same time [1, 10].
To specifically address analysis workflows of domain scientist who work with desktop PCs or laptops, we decided to consider only navigation methods that can be used seated. This again keeps everything easily integrable into the common workspace of an analyst and additionally is more comfortable and less fatiguing in the long run.
We performed a survey of potential candidates that fulfill the aforementioned requirements and evaluate them against each other and against a standard navigation device, here a gamepad (Pad). These methods include existing ones, namely a slightly adapted "Shake-Your-Head" (SYH) [11] and a leaning metaphor (Lean) [4]; an adapted "Walking-in-Place" metaphor (sWIP) [10] and finally two newly developed accelerometer pedal metaphors (AP and biAP).
2 RELATED WORK
Terziman et al. [11] showed that it is possible to realize WIP while seated and with very restricted hardware. Their method, called "Shake-Your-Head", records the head movements of a user with a webcam and deduces how these relate to the body/head movements while walking. Their implementation used a non-stereoscopic setup and provided only ground-based navigation.
*e-mail: {zielasko, freitag, weyers, kuhlen}@vr.rwth-aachen.de e-mail: sven.horn@rwth-aachen.de
IEEE Virtual Reality Conference 2016 19­23 March, Greenville, SC, USA 978-1-5090-0836-0/16/$31.00 ©2016 IEEE

Beckhaus et al. [1] used, among others, a simple navigation metaphor realized with a dance pad operated using the feet. Tracked with a Microsoft Kinect, Simeone et al. [9] use foot gestures to steer a virtual camera in a full 3D setting. The aforementioned techniques control translation and orientation with the feet, while Guy et al. [2] concluded that it is better when different interactions are controlled with uncorrelated body parts. Additionally, all approaches have in common that they use special tracking hardware.
Other methods do not use the feet but the whole body or upper parts. A popular metaphor is to navigate in the direction of the body's center of gravity, which is achieved by leaning the whole body [2] or just parts of it [2, 5] into the desired direction. The leaning metaphor is feasible also when seated as Kitson et al. [4] and Riecke et al. [8] showed with the NaviChair, and Beckhaus et al. [1] evaluated using the SwopperTM, both special joystick-like chairs without backrests. With navigation chairs like these it is possible to control up to three degrees of freedom, a 2D translation and turning around, which is enough for ground-based navigation. Guy et al. [2] evaluated different combination of pairs of body parts, excluding the hands, for a translation and rotation in a ground-based scenario.
3 NAVIGATION METHODS
We adapted or developed five different navigation methods. All methods use a tracked HMD, and some of them additionally a smartphone that is carried in the pants pocket and sends accelerometer and gravity sensor data (cf. Kim et al. [3]). The user is able to change the orientation of her virtual body around the yaw axis by turning her head to the left or right, and around the pitch axis by looking up or down. The only exception of this mapping is SYH. In the original implementation of Terziman et al. [11] the yaw of the virtual body was controlled by a roll of the real head and we kept the mapping this way.
When changing the orientation in the described way the ability to look around is lost, i.e., independently turning the virtual head from the virtual body. For this reason we define a deadzone in which the user is able to look around without moving the virtual head. The threshold for this deadzone was gathered in a user experiment. The translation of the virtual body is applied in the gaze direction, or rather head direction. The translation's speed is controlled differently in the methods: In sWIP the user has to repeatedly move one leg up and down. With biAP the user has to lift and lower the heel (forward and backward movement) while the forefoot stands on the ground to control the translations speed, similar to a turned accelerator pedal. Furthermore, in AP the foot by default is rested on the ground and can only be lifted (forward movement). In Lean the upper body's inclination is directly mapped to the velocity magnitude. Finally, we added the possibility to rotate the virtual body around the pitch axis to the original implementation of SYH.
4 EVALUATION
The goal of this work is to find a suitable hands-free navigation technique for seated immersive data analysis. For this purpose, we evaluated the methods presented in section 3 against each other and a standard 6-DOF gamepad-control given as baseline. The experiment took place at a regular office desk, which the user was seated in front

317

Figure 1: Results for the main dependent variables: Task completion time and error per task. The error bars show the 95% confidence intervals. A * denotes a statistical significance (p < .05).
of, on a rotatable and tiltable office chair with back- and armrests. The IVE was projected by an Oculus Rift DK2. For the evaluation we took a large 3D graph (5214 vertices and 6913 edges) and the user had to determine the shortest path between various pairs of vertices. The study has a 4×6 partial within-subject experimental design, i.e. every participant got 4 out of the 6 possible techniques, counter-balanced for order and frequency. For each condition, the participant first solved a training task followed by 6 recorded tasks. Additionally, the participants filled out a Kennedy's SSQ and a subjective questionnaire. 23 subjects (4 female and 19 male, mean age 29.5, SD = 10.4) finished the study, one participant prematurely canceled the experiment and was not considered in the final evaluation of the gathered data.
5 RESULTS
The averaged results of our dependent variables task completion time and error per task are depicted in Figure 1. There was a statistically significant difference in the task completion time between groups as determined by a one-way ANOVA (F(5,86) = 3.493, p = .006). Furthermore, a Tukey-HSD post-hoc test revealed that the task completion time using SYH was significantly slower than using Pad (p = .004) or biAP (p = .033). There were no statistically significant differences between the number of errors per task as determined by a one-way ANOVA (F(5,86) = .665, p = .651). Regarding the task completion time an additional independent-samples t-test between the group of methods that allow flying backwards, namely Pad, biAP and Lean (M = 28.1, SD = 13.5), and the ones that do not, AP, sWIP and SYH (M = 40.2, SD = 20.9), revealed a statistically significant difference (p = .001). There was no statistically significant difference between these groups regarding errors per task mean (p = .856). All but one participant used the possibility to fly backwards where possible.
6 DISCUSSION
Based on our results, we cannot recommend walking metaphors for seated 5-DOF navigation using an HMD. In sum, both methods were the slowest and were very poorly ranked in the subjective questionnaires. In our opinion, this is mainly due to two reasons. First, it is harder to immediately control the movement speed and thus, especially the moment of stopping and starting. This is usually compensated in scenarios with ground based navigation where this behavior results in a more natural feeling. Unfortunately, this is not true anymore for flying.
The accelerator pedal methods and the leaning method performed very well in general.
The possibility to move backwards was reported as positive when possible and as negative where is was not possible by a majority of participants. Additionally, a significant effect on the task completion time was able to confirm that, even when in our opinion moving backwards was not as important in the given task as potentially in

a real application. In sum, we recommend to use a method that allows moving backwards.
Confirming previous results [6], we also found a significantly higher SSQ score when the participants used the gamepad.
7 CONCLUSION
We collected a survey of different hands-free, HMD-based navigation methods that can be used while seated in an analyst's usual workplace. Additionally, all methods are easily and inexpensive to integrate into existing workplaces. In the given spatial environment, we found that a body-leaning metaphor and an accelerometer pedal metaphor performed best. Additionally, we could derive the recommendations to avoid using walking metaphors and including the possibility for traveling backwards. Furthermore, we could confirm observations made in prior studies that a higher simulator sickness occurs if using a non-embodied navigation method.
ACKNOWLEDGEMENTS
The authors would like to acknowledge the support by the Helmholtz portfolio theme "Supercomputing and Modeling for the Human Brain". The research leading to these results has received funding from the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreement n 604102 (HBP).
REFERENCES
[1] S. Beckhaus, K. J. Blom, and M. Haringer. Intuitive, Hands-Free Travel Interfaces for Virtual Environments. IEEE Virtual Reality Workshop: New Directions in 3D User Interfaces, pages 57­60, 2005.
[2] E. Guy, P. Punpongsanon, D. Iwai, K. Sato, T. Boubekeur, R. Abdrashitov, J. Yao, K. Singh, J.-M. Thierry, and J.-M. Thiery. LazyNav: 3D Ground Navigation with Non-Critical Body Parts. Proc. IEEE 3D User Interfaces, pages 43­50, 2015.
[3] J. Kim, D. Gracanin, and F. Quek. Sensor-Fusion Walking-In-Place Interaction Technique Using Mobile Devices. Proc. IEEE Virtual Reality: Short Papers and Posters, pages 39­42, 2012.
[4] A. Kitson, B. E. Riecke, A. M. Hashemian, and C. Neustaedter. NaviChair: Evaluating an Embodied Interface Using a Pointing Task to Navigate Virtual Reality. Proc. ACM Spatial User Interaction, pages 123­126, 2015.
[5] J. J. LaViola Jr, D. A. Feliz, D. F. Keefe, and R. C. Zeleznik. Hands-Free Multi-Scale Navigation in Virtual Environments. Proc. ACM Interactive 3D Graphics, pages 9­15, 2001.
[6] G. Llorach, A. Evans, and J. Blat. Simulator Sickness and Presence Using HMDs: Comparing Use of a Game Controller and a Position Estimation System. Proc. ACM Virtual Reality Software and Technology, pages 137­140, 2014.
[7] E. D. Ragan, R. Kopper, P. Schuchardt, and D. A. Bowman. Studying the effects of stereo, head tracking, and field of regard on a small-scale spatial judgment task. IEEE Transactions on Visualization and Computer Graphics, 19(5):886­896, 2013.
[8] B. E. Riecke and D. Feuereissen. To Move or Not to Move: Can Active Control and User-driven Motion Cueing Enhance Self-motion Perception ("Vection") in Virtual Reality? Proc. ACM Applied Perception, pages 17­24, 2012.
[9] A. Simeone, E. Velloso, J. Alexander, and H. Gellersen. Feet Movement in Desktop 3D Interaction. Proc. IEEE 3D User Interfaces, pages 71­74, 2014.
[10] M. Slater, M. Usoh, and A. Steed. Taking steps: The influence of a walking technique on presence in virtual reality. ACM Transactions on Computer-Human Interaction, 2(3):201­219, 1995.
[11] L. Terziman, M. Marchal, M. Emily, F. Multon, B. Arnaldi, and A. Le´cuyer. Shake-Your-Head: Revisiting Walking-in-Place for Desktop Virtual Reality. Proc. ACM Symposium on Virtual Reality Software and Technology, pages 27­34, 2010.

318

