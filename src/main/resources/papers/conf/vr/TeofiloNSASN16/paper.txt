Bringing Basic Accessibility Features to Virtual Reality Context

Mauro Te´ofilo
Samsung Research Institutee
Amazon Federal University

Josiane Nascimento
Samsung Research Institute
Andre L. Souza¶
The University of Alabama

Jonathan Santos
Samsung Research Institute
Daniel Nogueira
Samsung Research Institute

Yves Albuquerque§
Samsung Research Institute

Abstract
Virtual reality is an experience, often generated by computer, which brings immersive environments that can be interacted with. Since 2014, the spread of VR technology created a content demand as well as new paradigms of interactions. One of the key aspects of these new paradigms is that they consider the use of virtual reality by visually impaired people. In HCI, accessibility features are special computer functions that help people with disabilities to use technology more easily. This paper introduces Virtual Reality (VR) basic scenarios of accessibility tools like zooming, negative colors, auto reading, text-to-speech, subtitles, cursor based on context, and so on. The proposed solutions were designed based on accessibility features already in use by other platforms.
Index Terms: H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities--; K.4.2 [Social Issues]: Assistive technologies for persons with disabilities--;
1 Introduction
Virtual Reality (VR) is an environment focused on visual immersive experiences. It refers to the use of interactive simulation to provide users with opportunities to engage in environments that may appear and feel similar to real-world objects and events and that may provide the user with a subjective feeling of presence[1].
New interaction paradigms emerge over unexplored interactive medium. Such paradigms include the use of this media by impaired people. The current work focused mainly on visually impaired people and hearing loss people, however, also can serve as to other targets.
Due to its supportive use for other applications, we used a Launcher app of Gear VR as a case-scenario running into Samsung Gear VR. Gear VR is a portable VR solution developed by Oculus and Samsung that use cellphone screen technology and a super-low-latency sensor tracking.
2 Porting Accessibility Features to VR Context
According to Hersh and Johnson[2] assistive technology can be any equipment, device and system used to overcome the social infrastructure and other barriers experienced by disabled individuals and prevent their full and equal participation in all aspects of society. Web Content Accessibility
e-mail: mauro.t@samsung.com e-mail:josiane.p@samsung.com e-mail:js.santos@samsung.com §e-mail:yves.ja@samsung.com ¶e-mail:alsouza@ua.edu e-mail:daniel.nog@samsung.com
IEEE Virtual Reality Conference 2016 19­23 March, Greenville, SC, USA 978-1-5090-0836-0/16/$31.00 ©2016 IEEE

Guidelines (WCAG) defines these technologies as hardware or software that acts as a user agent to provide functionalities to meet the users needs with disabilities that are not offered by mainstream systems.
The basic features tested in VR are standard assistive technologies found in web, mobile and other communication media: zoom, inverted colors, speech recognition, auto reading and captions. The chosen scenario was a launcher app of Gear VR device, implemented in Unity engine, that uses a visual theme of sea and sky colors with day-night cycle system according to device clock and presents a user interface content of applications, trends (new applications), clock time, battery level and apps arrangement options (sort by).
2.1 Accessibility Menu
The first proposal of this pilot project was to experiment an easy access to activate the accessibility menu where the features can be activated and deactivated. Since the most of platforms place this section in settings screen, the challenge of this menu access is in the 3D world the space references are completely different from 2D media. For this reason three ways to access the menu were designed having in mind accessibility concepts already in use by 2D media: by hand gesture, by look down in a button and by speech recognition of the spoken word Accessibility. The objective is already to start the experience using assistive technologies.
2.2 Zooming adaptation to VR
Zooming is an alternative way of present some system with magnified content. Aimed to improve the visual readability of rendered text and images of applications grid in launcher application, this technology was used with FOV adjustment seeing that it is not recommended to use simple zooming. To interact with zoom-in or zoom-out, the player taps three times on touchpad or says zoom-in or zoom-out in specific direction of cursor.
2.3 Inverted Colors to VR
Other tested feature was the inverted colors. When user activates the color inversion in menu, immediately the entire launcher application is updated, consequently all apps used too. If user wants to turn off the functionality the menu needs to be accessed again.
2.4 Speech recognition, Auto reading and Captions
The speech recognition in Gear VR works in similar way as in mobile devices, for this prototype was considered an offline vocabulary for preset interactions.
Basically three general commands work to navigate in launcher: applications names, tabs names (apps and trends) and applications arrangement (alphabetic or recently used).
2.5 Auto-reading (Screen Reader)
Like zooming, this feature depends on the cursor direction and can be used by partially blind and fully blind people.

293

Just like other medias, when the user hovers the cursor over an element, audio information is provided.
2.6 Captions
Captions are generally found in video players and are focused in dialogues, audio effects, or any other sound emission. In this pilot the captions were treated as a font magnifier with big font body in interactive elements and in the head information. The reason of this experiment is due to aliasing of screen that affects specially fonts, an issue that is also uncomfortable for people without disabilities.
The captions focus is to improve the visualization of font spacing and more definition of each type.
3 Discussion and Preliminary Results
We developed a pilot using some concepts described below in this section. We also presented some notes and evidence that we obtained by analyzing the user-disabled feedback.
We realized that the focus on actions between the intention and realization of something is a valuable source to understand new possibilities regarding how user interfaces could provide a better experience for impaired people. In a user experience flow, we found three major steps: The goal, what the user wants to do, related desires into the user experience. The tasks, what the user does to carries out his will. And finally, the results of user actions provided by UI. For example, when the user wants to know what time is it he has an Intention. When he looks to his watch he has does an action. Lastly, when he reads the display watch and understands what time it is, he got a interpretation. In this example we can infer just one action: look the watch. However one intention can cause several actions.
All actions in interface are inputs. A user interface receives these inputs through sensors. For example, often a smartphone has several sensors like: touchscreen, physical microphone, accelerometer, gyroscope, etc. In turn, all results or feedbacks of each user action are outputs. Adopting the previous example, a touchscreen can be understood as either input as output. For purposes of this study we understand "interaction" as one or more output/input stimulus. The interactions are built using technologies that explore at least one human sense and some physical and mental conditions.
There are mental models when the user interface includes features that just can be accessed through actions that have a dependence of same kind of human abilities. Once the people who are deficient of these abilities are unable to access the desired feature.
We experienced better results in user interfaces that provide multiple ways to access a feature and favors the diversity of human physical and mental conditions.
The multiple ways to the user reach a feature is not enough to ensure a good user experience to impaired people if there is no coherence between the interactions and the user target that the interface aims to meet. Thus, offer resources that require actions based on a specific human sense or a specific health condition to people who lack these abilities results an unsuccessful user experience.
We understand user autonomy as the user's ability to perform all functions that he wants without having to be helped by others. According to the principles of user autonomy the user must be able to enable any assistive tools by him.
Nowadays it's not possible to turn on or turn off any accessibility tools available in Android Operation System while the user is using the Samsung Gear VR. However, if the user enables negative colors, colors adjustments or zoom, these

features may work inside the virtual reality context. It happens because Gear VR uses the whole infrastructure of a Samsung Smartphone, and some cases and in some cases inherits some platform core aspects.
Therefore, we understand the need to enable the user turn on and turn off these features inside the virtual reality context. So we developed in the prototype different ways to enable or disable these features.
Some accessibility features require a fast switch; a quick way to switch between states. For example, often who uses negative colors (or inverted colors) switches between ON and OFF several times according to the context or scene. Considering this aspect, we included in pilot shortcuts to handle the states of some accessibility features that have this transient behavior.
Considering a environment where the user set the direction of his navigation using a cursor controlled by his head movements, we designed voice actions as an alternative for virtual buttons and taps. It's not just a way to include who can't see or tap a virtual button as a confortable option for everybody access a content or a resource inside the 3D environment.
4 Conclusion
We found strong evidence that all proposed solutions can be use in VR context aiming to bring accessibility to VE. Some technical issues and limitations emerge when re-interpreting the elected solutions into VR context but, in general, the main concern was more bounded by the huge scope of possibility than by restrictions.
Besides being considered a predominant visual experience, we found evidences that VR can be expanded to, not only include impaired people as target, but also, when several constraints of common displays are put aside, enhance the current accessibility tools expanding technology and information access.
4.1 Future Works
After the pilot results, we realized other solution options as launcher for impaired in specific layout format, as font and icons size also can be considered, according to results sometimes zoom can lose the spatial references. Other point to be investigated is a contextual cursor. For color blindness also there are a initiative to investigate shaders with gray scale and red, green and blue variations to test and validate with these specific visual impaired people.
5 Acknowledgements
Part of the results presented in this paper were obtained through research on project titled "VR Accessibility", sponsored by Samsung Eletrnica da Amaznia Ltda. under the terms of Brazilian federal law No. 8.387 (art. 2) / 91.
References
[1] N. Ghali, O. Soluiman, N. El-Bendary, T. Nassef, S. Ahmed, Y. Elbarawy, and A. Hassanien. Virtual reality technology for blind and visual impaired people: Reviews and recent advances. In T. Gulrez and A. Hassanien, editors, Advances in Robotics and Virtual Reality, volume 26 of Intelligent Systems Reference Library, pages 363­385. Springer Berlin Heidelberg, 2012.
[2] M. Hersh and M. A. Johnson. Assistive Technology for Visually Impaired and Blind People. Springer Publishing Company, Incorporated, 1st edition, 2008.

294

